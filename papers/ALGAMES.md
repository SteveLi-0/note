# ALGAMES: A Fast Solver for Constrained  Dynamic Games
Simon Le Cleac’h,  Mac Schwager, Zachary Manchester

本文重点在于相关工作。

## 摘要

动态博弈是处理多个互动行为体控制问题的有效范式。本文提出了一种称为ALGAMES的求解器（增强拉格朗日博弈论求解器），用于解决包含多个行为体的轨迹优化问题，支持一般的非线性状态和输入约束。其创新之处在于，通过拟牛顿根查找算法满足一阶最优性条件，并使用增强拉格朗日公式严格强制执行约束。我们在高度互动的自动驾驶场景中评估了此求解器的性能，通过蒙特卡洛模拟检验其鲁棒性。与基于DDP的前沿方法相比，该方法在解决复杂问题（如三辆车的坡道并入）时快三倍。此外，基于该算法的模型预测控制（MPC）实现，在复杂的自动驾驶场景中实现了高于60 Hz的实时更新频率。

## 引言


在包含多个行为体的环境中控制机器人是一项复杂的任务。传统方法通常采用“先预测后规划”的架构，首先预测其他行为体的轨迹，然后将这些轨迹视为不可改变的障碍输入规划器。然而，这种方法存在局限，因为它忽视了机器人轨迹对其他行为体的影响。此外，这种方式可能会导致所谓的“冻结机器人”问题，即当规划器发现所有通往目标的路径均不安全时，机器人陷入停滞。

因此，为了更准确地反映场景中所有行为体的互动特性，机器人需要在规划自身轨迹的同时预测其他车辆的轨迹。ALGAMES提供了这样一个联合轨迹预测与规划的工具，通过将所有行为体视为纳什动态博弈中的玩家。我们设想ALGAMES可以在机器人上以滚动预测的方式在线运行，在每次迭代中根据周围行为体的反应性特征为机器人规划轨迹。

在包含多个互动行为体的场景中进行联合轨迹预测与规划可通过动态博弈得到良好的描述。处理多行为体规划问题中的博弈论特性是一个关键问题，具有广泛的应用前景。例如在自动驾驶中，坡道并入、换道、交叉路口通过和超车等操作都涉及一定程度的博弈论互动。其他潜在的应用还包括在人群中导航的移动机器人，如包裹配送机器人、导游机器人或家庭服务机器人；在工厂中与人类互动的机器人，如移动机器人或固定基多连杆机械臂；以及一些竞争性场景，如无人机和赛车竞技。

在本研究中，我们寻求解决多行为体的约束一般和博弈问题。在动态博弈中，每个玩家的策略是一系列决策。需要注意的是，不同于传统的优化问题，非合作博弈并没有唯一的“最优”解。根据博弈的结构、玩家之间的不对称性等因素，可能存在多种解的概念。在本研究中，我们寻找纳什均衡解。这类均衡模型中，所有玩家被平等对待，在该均衡状态下，任何一个玩家都无法通过单方面改变其策略来降低自身的成本。

我们的求解器旨在寻找多行为体动态博弈的纳什均衡，并能够处理一般的非线性状态和输入约束。这对于机器人应用尤为重要，因为这些行为体通常会通过避免相互碰撞或避免与环境发生碰撞来进行互动。此类互动通常最自然地表示为（通常是非线性）状态约束。这一特性使得面向机器人的博弈论方法在处理上区别于其他领域的博弈论方法，如经济学、行为科学和鲁棒控制等。在这些领域，行为体之间的互动通常直接表现在目标函数中，这些博弈一般没有状态或输入约束。在数学文献中，带有约束的纳什均衡被称为广义纳什均衡。因此，本文提出了一种增强的拉格朗日求解器，用于寻找特别适用于机器人应用的广义纳什均衡。

我们的求解器假定玩家是理性的行为体，并努力使其成本最小化。这种理性行为的表达方式采用了纳什均衡的一阶必要条件，类似于优化中的Karush-Kuhn-Tucker（KKT）条件。通过依赖增强拉格朗日方法来处理约束，该求解器能够以实时速度求解多个行为体间的高度互动场景。例如，解决一个三辆自动驾驶汽车在高速公路并入场景中的纳什均衡仅需90毫秒。我们主要的贡献包括：

1. 提出一种面向动态博弈的通用求解器，用于识别广义纳什均衡策略。
2. 提供一种实时的MPC实现方案，能够处理噪声、扰动及碰撞约束。
3. 与iLQGames进行速度对比，ALGAMES在达到固定约束满足条件的前提下，找到纳什均衡的速度是iLQGames的三倍。

## 相关工作
### A. 均衡选择

近期的多行为体动态博弈求解工作可以根据所选择的均衡类型进行分类。若干研究选择搜索Stackelberg均衡，该均衡建模了玩家之间的信息不对称。这类方法通常适用于双人博弈情境，包括一名领导者和一名跟随者。领导者首先选择其策略，然后跟随者根据领导者的策略选择最优响应。然而，纳什均衡并未引入玩家间的层级关系，每个玩家的策略都被认为是对其他玩家策略的最佳响应。正如中所指出的，寻找开环Stackelberg均衡策略在一些简单例子中可能失效。在自动驾驶情境中，例如当玩家的成本函数仅依赖于自身的状态和控制轨迹时，解可能变得过于简单：领导者忽略彼此间的碰撞约束，而跟随者则只能适应领导者的策略。这种行为可能导致领导者的策略过于激进（或跟随者过于被动），并未体现问题的博弈论特性。

在若干研究中，纳什均衡被广泛探讨。我们也选择以寻找纳什均衡的方式进行研究，因为该均衡更适合对称的多机器人交互情境。实际上，在求解开环策略时，我们观察到纳什均衡相较于Stackelberg均衡展现出更加自然的行为模式。

### B. 博弈论轨迹优化

机器人学领域中提出的用于求解博弈论均衡的算法主要可以分为四类：首先，一些算法旨在通过分解来找到纳什均衡，例如Jacobi或Gauss-Siedel方法。这些算法基于一种迭代的最佳响应方案，在这种方案中，玩家轮流改善其策略，假定其他行为体的策略保持不变。这种方法易于理解，并且在玩家数量较多时具有良好的扩展性。然而，这类算法的收敛性尚不明确，且在捕获问题的博弈论特性时需要特别注意。此外，为了使纳什均衡求解收敛，可能需要多次迭代，而每次迭代都可能是代价高昂的轨迹优化问题，导致计算时间过长。

其次，另一类算法基于动态规划。在中，通过动态规划计算Markovian Stackelberg策略。这种方法似乎能够捕捉到自动驾驶中的博弈论特性。然而，动态规划受到维度诅咒的限制，因此实际实现往往依赖简化的动力学模型并对状态和输入空间进行粗略离散化。为了弥补这些简化，一种较低级的规划器会在马尔可夫Stackelberg策略下的状态值指导下运行。这种方法的时间复杂度随状态维度呈指数增长，目前仅在双人博弈情境下得以验证，增加玩家数量可能阻碍其在实时应用中的应用。相比之下，我们提出的方法的时间复杂度在玩家数量上呈多项式增长（见IV-E节）。

第三类算法类似于微分动态规划，最初用于鲁棒控制，后来被应用于博弈论问题。这种方法在玩家数量上呈多项式增长，且足够快速，可以在模型预测控制（MPC）方式下实时运行。然而，这类方法并不原生支持约束，碰撞规避约束通常通过大的惩罚项来处理，这可能导致数值不适定，从而影响求解器的鲁棒性或收敛速度。此外，这种方法在轨迹效率与规避其他玩家碰撞之间存在权衡。

最后一种算法类似于轨迹优化中的直接方法。Di提出了一种基于一阶分裂方法的算法，其已知的收敛速率为线性。其实验结果表明，该算法在103到104次迭代后收敛。另一种基于牛顿方法的不同方法已被提出，但该方法仅限于无约束的动态博弈。我们的求解器属于这种方法的范畴，它同样依赖于二阶牛顿型方法，但能够处理一般的状态和控制输入约束。此外，我们展示了在相对复杂的问题中，该方法通常在不到102次迭代内收敛。

### C. 广义纳什均衡问题

如前所述，我们专注于寻找多行为体博弈中带有共享状态约束的纳什均衡（例如碰撞规避约束）。因此，这类问题属于广义纳什均衡问题（GNEPs）。运筹学领域对于GNEP有丰富的文献。一些精确惩罚方法被提出用于求解GNEP，复杂的约束（如耦合玩家策略的约束）通过惩罚项处理，从而能够为所有玩家共同求解多行为体博弈。然而，这些精确惩罚方法需要最小化非光滑目标函数，实际中收敛速度较慢。

在类似思路中，Pang等人提出了一种基于增强拉格朗日的惩罚方法。该方法将增强拉格朗日公式转化为一组KKT条件，包括互补性约束。得到的约束满足问题通过一个特定问题的线性互补问题（LCP）求解器来求解，利用了该问题的线性特性。相比之下，我们的求解器并非针对特定实例设计，可以求解一般的GNEP。它借鉴了增强拉格朗日公式，避免了在目标函数中引入非光滑项，从而实现快速收敛。此外，这一公式避免了病态问题的出现，提高了求解器的数值鲁棒性。

以下是不同博弈方法的总结以及其优势与劣势：

### 1. Stackelberg均衡
**优势**：适用于领导者-跟随者结构的双人博弈，能够体现信息不对称，尤其适合某些策略优先级明确的应用场景。
**劣势**：在自动驾驶等情境中，当领导者忽略碰撞约束时，解可能过于简单，导致策略过于激进或跟随者过于被动，不完全体现博弈论特性。

### 2. 纳什均衡
**优势**：适用于对称的多行为体博弈，尤其适合机器人交互场景。纳什均衡方法在开环策略中能展示出更加自然的行为模式。
**劣势**：当使用分解算法（如Jacobi或Gauss-Siedel方法）时，收敛性不确定，迭代过程计算量大，可能导致时间开销大。

### 3. 动态规划（用于Markovian Stackelberg策略）
**优势**：在双人博弈情境中能捕捉博弈论特性，适用于自动驾驶等动态博弈问题。
**劣势**：受维度诅咒限制，状态维度增加将导致计算复杂度指数增长，难以扩展至多玩家博弈，适用性受到限制。

### 4. 微分动态规划
**优势**：计算复杂度随玩家数量呈多项式增长，能够在模型预测控制（MPC）框架下实时运行。
**劣势**：不原生支持约束，碰撞规避等约束需通过惩罚项处理，可能引起数值不适定，影响求解器鲁棒性与收敛速度，需在轨迹效率与规避碰撞之间进行权衡。

### 5. 直接方法中的一阶分裂与牛顿方法
**优势**：一阶分裂方法具有线性收敛速率，迭代次数较少；基于牛顿方法的求解器可以处理一般的状态和控制输入约束，收敛速度更快且适用于更复杂的问题。
**劣势**：部分方法仅限于无约束的动态博弈，一阶分裂方法需大量迭代（103至104次），计算资源消耗大。

### 6. 广义纳什均衡问题（GNEP）
**优势**：适用于多行为体博弈中带有共享状态约束的问题，如碰撞规避。增强拉格朗日法避免非光滑项的引入，提高了数值鲁棒性和收敛速度。
**劣势**：精确惩罚方法求解GNEP时需最小化非光滑目标函数，可能导致收敛速度较慢；对于复杂的耦合约束，计算难度增加。