# 【机翻】Receding Horizon Motion Planning for Automated Lane Change and Merge Using Monte Carlo Tree Search and Level-K Game Theory

## 摘要

在自动驾驶中，运动规划和预测周围环境的未来状态是主要挑战之一。在变道和并线操作中，预测邻近车辆在未来的反应尤为重要，尤其是在缺乏车间通信（如V2V、V2X等）的情况下。现有的人类驾驶模型、概率方法、基于规则的技术和机器学习方法对此问题有所涉及，但未完全聚焦于车辆的行为特征。此外，预测框架还需能够快速向路径规划器提供未来状态的估计。构建一个能够快速考虑车辆间交互的结构是本研究的主要动机。在本文中，**我们提出了一种基于蒙特卡罗树搜索（MCTS）的快速递归预测算法**，用于高速公路场景中的实时路径规划。该算法受到最新研究成果的启发，**采用层级K游戏理论框架来预测周围车辆的策略**。仿真结果显示，该方法具备快速计算的良好效果。

## 1. 引言

近年来，变道和并线操作在自动驾驶领域引起了广泛关注。主要挑战在于如何在不影响交通流畅的情况下，安全、舒适地执行这些操作。这一问题的难点在于，周围车辆的未来意图通常未知，同时路径规划在计算上也非常复杂，因为需要在非凸的可行域中进行规划，并且需满足多种硬性和软性约束。本文的主要目标是开发一种能够实时实现的运动规划算法，该算法能够通过博弈论方法考虑与邻近车辆的交互。

在多车可能发生碰撞的场景中，一条成功的轨迹不仅能够避免碰撞，还需考虑能耗效率和操作时长。通过递归预测方式，可以利用周围环境的更多信息，从而提升规划轨迹的安全性和最优性。一种常见的获取这些信息的方法是通过代理间的通信。然而，尽管通信方法在自动驾驶中已展示出较好的效果，通信丢失、延迟或网络攻击可能会导致不良后果。此外，在可预见的未来，很难确保所有周围车辆都具备联网能力。在缺乏通信的情况下，可使用人类驾驶行为的解析模型（如MOBIL模型）或概率模型，也可以应用机器学习和人工智能技术来应对有其他代理存在的路径规划问题。然而，这些方法在复杂性、计算成本以及所需的大量训练和验证数据方面都面临挑战。

为降低计算复杂度，本文提出使用蒙特卡罗树搜索（MCTS）在高速公路场景中寻找接近最优的车辆轨迹。受到最新研究成果的启发，我们采用层级K博弈论来根据其他车辆的策略深度预测其轨迹。在其他研究中，Li等人结合层级K方法和强化学习，创建了一个框架用于不确定环境中的预测和运动规划。类似方法还被应用于无信号交叉口等其他场景。

蒙特卡罗树搜索（MCTS）是一种将蒙特卡罗方法与游戏树搜索相结合的随机采样搜索技术，常用于处理状态空间部分可观测的策略性游戏，如《吃豆人》、《围棋》和《Kriegspiel》等。这类场景的不确定性使决策变得更加困难，特别是在需要快速决策时。MCTS通过模拟大量可能的策略并系统性地评估它们的结果，最终选择由代理执行的最佳动作。

在多代理环境中，每个代理都在选择一系列最优动作（即策略）来实现某些目标，这种环境下每个代理的动作都会影响其他代理。例如，在高速公路驾驶场景中，车辆之间的碰撞是一个主要关注点。预测每个参与代理在一段时间内的动作序列，并在自车的采样过程中结合这种预测，有助于减少冲突互动的可能性。在整合层级K博弈论时，原则上每个路径规划器都假设与其交互的驾驶者具备较低层次的策略能力，并利用蒙特卡罗树搜索（MCTS）预测并评估这些驾驶者在规划周期内的可能动作序列。最终，规划器执行另一轮MCTS，以在考虑其他代理预测动作的基础上找到符合约束条件的自车轨迹。

本文的其余部分组织如下：第二节A部分描述了用于捕捉车辆动力学的动态模型以及动作空间。第二节B部分介绍了奖励函数的定义。第三节详细介绍了蒙特卡罗树搜索（MCTS），第四节则阐述了层级K博弈论及其与MCTS的整合。第五节展示了仿真结果，最后在第六节给出了结论。

## 2. 建模

### II. 建模框架

#### A. 动力学模型和动作空间

由于本研究主要关注于高速公路中的自动驾驶高层控制器的开发，因此采用了简化的运动学模型来描述高速公路上车辆的运动。离散化的状态空间模型如下所示：

$$
x_{k+1} = x_k + v_k \cdot \cos(\theta_k) \cdot \Delta t
$$

$$
y_{k+1} = y_k + v_k \cdot \sin(\theta_k) \cdot \Delta t
$$

$$
v_{k+1} = v_k + a_k \cdot \Delta t
$$

其中，$ x $ 和 $ y $ 表示纵向和横向位置，$v$ 和 $ a $ 分别表示纵向速度和加速度，$\theta$ 为偏航角，$\omega$ 为偏航率。此模型的控制输入（即动作）由$(a, \omega)$ 确定，并由规划器选择。动作从一个离散化的动作空间中选择，动作空间的定义如下表所示：

| 动作编号 | 描述         | 加速度 (m/s²) | 偏航率 (rad/s) |
| -------- | ------------ | -------------- | -------------- |
| 1        | 保持当前速度 | 0.00           | 0.00           |
| 2        | 轻刹车       | -1.50          | 0.00           |
| 3        | 轻加速       | 1.50           | 0.00           |
| 4        | 中度刹车     | -3.50          | 0.00           |
| 5        | 高速加速     | 2.50           | 0.00           |
| 6        | 重刹车       | -5.00          | 0.00           |
| 7        | 轻左转       | 0.00           | π / 4         |
| 8        | 轻右转       | 0.00           | -π / 4        |
| 9        | 大左转       | 0.00           | π / 2         |
| 10       | 大右转       | 0.00           | -π / 2        |
| 11       | 加速并左转   | 1.50           | π / 4         |
| 12       | 加速并右转   | 1.50           | -π / 4        |
| 13       | 刹车并左转   | -1.50          | π / 4         |
| 14       | 刹车并右转   | -1.50          | -π / 4        |

#### B. 奖励函数

在递推视界的规划过程中，每一步算法根据状态方程（1）确定当前状态。然后，它通过自车和交通的状态来计算动作的即时奖励，奖励函数如下所示：

$$
r(a_k, s_k) = \pi_1 \cdot \Gamma_{\text{collision}} + \pi_2 \cdot \Gamma_{\text{safe dist.}} + \pi_3 \cdot \Gamma_{\text{off-road}} + \pi_4 \cdot \Gamma_{\text{bet. lines}} + \pi_5 \cdot \Gamma_{\text{speed}} + \pi_6 \cdot \Gamma_{\text{yaw}} + \pi_7 \cdot \Gamma_{\text{decel.}}
$$

其中，$ \Gamma $ 表示我们选择的七个元素，用于表示约束违反的测量指标，$\pi$ 表示根据其重要性分配的权重。规划视界内的累积奖励由以下公式决定：

$$
\Re(\rho_i)=\sum\limits_{k=0}^{m-1}d^k.r_i(a_k,s_k)
$$

其中，$ m $ 为规划视界的长度，$ ℘_i $ 表示为第 $ i $ 个智能体通过 MCTS 模拟的策略，$ d \in (0, 1) $ 为折扣因子，用于降低视界末端瞬时动作的影响。以下对每个 $ \Gamma $ 进行详细解释。请注意，每个 $ \Gamma $ 都被缩放到 [0,1] 区间，这使得算法脚本中的权重调优更加方便。

1. $\Gamma_{\text{collision}}(s_i, S'_i) $：这是一个分段函数，基于智能体状态 $ s_i $ 和交通状态 $ S'_i $ 返回值。当动作执行后，智能体未与任何对象或其他智能体碰撞时返回1，否则返回0。此函数奖励不导致碰撞的动作。
2. **$ \Gamma_{\text{safe dist.}}(s_i, S'_i) $**：这是另一个分段函数，基于 $ s_i $ 和 $ S'_i $。当智能体的安全边界未与任何其他智能体重叠时返回1，否则返回0。
3. **$ \Gamma_{\text{off-road}}(s_i) $**：仅基于智能体状态 $ s_i $ 的函数。如果智能体未偏离道路区域则返回1，否则返回0。
4. **$ \Gamma_{\text{bet. lines}}(s_i) $**：此函数在智能体保持在高速公路的车道线之间时返回1，否则返回0。
5. **$ \Gamma_{\text{speed}}(s_i) $**：该函数奖励将速度保持在期望速度 $ v_{\text{dsrd.}} $ 附近的行为，其定义为：

$$
\Gamma_{\text{speed}}(s_i) =
\begin{cases}
1, & \text{如果 } |v - v_{\text{dsrd.}}| \leq 1 \\
0, & \text{如果 } |v - v_{\text{dsrd.}}| > v_{\text{dsrd.}} \\
1 - \frac{|v - v_{\text{dsrd.}}|}{v_{\text{dsrd.}}}, & \text{否则}
\end{cases}
$$

6. **$ \Gamma_{\text{yaw}}(s_i) $**：此函数奖励保持车辆沿纵向方向行驶，定义如下：

$$
\Gamma_{\text{yaw}}(s_i) =
\begin{cases}
1, & \text{如果 } |\psi| \leq 0.01 \\
1 - \frac{4}{\pi} |\psi|, & \text{如果 } 0.01 < |\psi| \leq \frac{\pi}{4} \\
0, & \text{如果 } |\psi| > \frac{\pi}{4}
\end{cases}
$$

7. **$ \Gamma_{\text{decel.}}(s_i) $**：在没有临近交通的情况下，车辆减速时此函数返回0。临近交通是一个观测状态，定义为自车前方预定范围内存在任何车辆。当在无交通的情况下不减速或在接近缓慢交通时减速，该函数返回1（即奖励该动作）。

以下是论文中“III.蒙特卡罗树搜索”（III. MONTE CARLO TREE SEARCH）部分的完整翻译：

### III. 蒙特卡罗树搜索

并线和变道操作类似于一个博弈，其中两个或更多的玩家相互互动。在博弈的每个状态（在博弈论文献中称为“节点”）中，玩家尝试利用某个动作以最大化所得到的奖励。MCTS（蒙特卡罗树搜索）算法在每一步操作之前通过一系列虚拟策略模拟进行迭代。算法计算并记录每个模拟策略的累积奖励。在每次迭代中，MCTS系统地使用该奖励来选择虚拟策略。这个创新特性是MCTS的一大显著特点。依赖“大数法则”，MCTS算法会收敛到预期能够获得最高累积奖励的策略。在我们特定问题的每个状态下，从表1中选择一个动作。为了在执行每一步操作前找到视界内的最优策略，需要在暴力计算方法中评估 $ 14^m $ 种动作组合。即便是相对较短的视界长度，这也是一个庞大的数字。即使使用贝尔曼最优性原理，所需的计算量仍然很大。例如，当每个四个状态被量化为 $ n_x $ 个值时，每个阶段的奖励评估次数为 $ (n_x)^4 \times m \times 14 $。即使使用较粗略的量化（例如 $ n_x = 10 $）并使用 $ m = 10 $ 的预测视界，每个互动智能体的奖励函数评估也会达到140万次。在这种情况下，预测邻近车辆在自车轨迹规划后的轨迹可能会导致很高的计算成本。换句话说，算法的时间复杂度和空间复杂度都很高，即便是在不太复杂的动作空间和中等长度的视界中。因此，应用MCTS作为一种系统且启发式的机制，遵循大数法则，有可能解决计算问题，尽管这会牺牲一定的最优性。

MCTS的递归过程包含以下四个主要步骤，简要描述如下。更详细的描述可以参考文献[12, 16]。

1. **选择（Selection）**：选择从动作空间中选择动作来形成要模拟的策略。此步骤包括两种选择模式：树策略（tree policy）和默认策略（default policy）。树策略仅在树的现有节点中操作。在每次迭代中，树策略从根节点（即当前的MCTS代理状态）开始，穿过搜索树并选择平衡以下两方面的动作：1）较高奖励动作的利用；2）较少选择动作的探索。当树策略选择一个未被模拟的非终端节点（即递推视界的最后一步）时，默认策略触发，为视界的剩余部分随机选择动作。假设MCTS算法在一个包含 $ n_{\alpha} $ 个动作的视界长度 $ m $ 上运行决策。根据树策略，算法选择动作序列，例如 $ ℘_{\text{tree}} = \{a^0_2, a^1_9, \dots, a^{m-l-1}_5, a^{m-l}_3\} $，其中下标表示动作空间中的动作索引，上标表示动作在序列中的索引，且小于 $ m $。在此示例中， $ a^{m-l}_3 $ 是一个未被模拟的叶节点。叶节点指的是没有子节点的节点。从视界的这一点到终点，默认策略选择动作，直到达到终端状态，输出随机选择的动作序列，例如 $ ℘_{\text{default}} = \{a^{m-l+1}_7, a^{m-l+2}_1, \dots, a^{m-2}_2, a^{m-1}_0\} $。此步骤的输出是递推视界的动作序列：

$$
℘ = ℘_{\text{tree}} \cup ℘_{\text{default}}
$$

树策略选择的过程使用树的上置信界（UCT）来平衡探索与利用。在MCTS范围内已研究了不同的UCT函数。其最常见的形式如下所示：

$$
UCT = \frac{r_a}{n_a} + c \cdot \sqrt{\frac{\log N}{n_a}}
$$

其中，$ N $ 是当前的MCTS模拟总次数，$ n_a $ 是树策略选择动作 $ a $ 的模拟总次数，$ r_a $ 是动作 $ a $ 的累积奖励。参数 $ c $ 是UCB常数，用于平衡对动作 $ a $ 的探索和利用。公式（7）中的第一项对相对奖励较高的动作有更高的值，公式的第二项对与树中总模拟次数相比利用率较低的动作有更高的值。树策略选择具有较高UCT值的子节点（或动作）。例如，在图1所示的选择步骤中，中心节点被选中，尽管它不是奖励最高的节点，因为它的UCT值较高（UCT=25.44）相比于奖励最高的节点（UCT=24.51）。

2. **扩展（Expansion）**：如果树策略选择了一个被模拟过的叶节点，MCTS会执行此步骤。扩展步骤会向搜索树的当前结构中添加一组新的子节点。扩展后，树策略从新的子节点中选择一个节点。
3. **模拟（Simulation）**：此步骤执行所选策略，并使用公式（1）确定状态序列，通过公式（3）计算所选策略的累积奖励 $\Re(℘) $。图2展示了在图1所示选择和扩展步骤之后的模拟步骤示例。
4. **反向传播（Backpropagation）**：作为单次MCTS迭代的最后一步，算法通过选定的策略进行反向传播，并更新树策略节点的属性。图3展示了此步骤的示例。为明确起见，反向传播算法在树策略的所有节点上执行以下任务：

   - 将计算出的累积奖励 $\Re(℘) $ 添加到树策略中每个节点的当前存储奖励 $ r_a $。
   - 将 $ n_a $ 和 $ N $ 均增加一个单位。

总结来说，在递推视界方法下的变道或并线操作中，在每一步操作时，MCTS会在固定的视界内执行大量模拟，从表1中选择动作并根据公式（1）推进状态。一旦满足终止条件，算法会输出奖励最高的策略以供应用。最终，自动驾驶车辆执行动作序列中的第一个动作并进入后续状态。视界也向前推进一步。算法不断重复相同的步骤，完成后续的操作。算法1展示了MCTS算法的高层结构伪代码。在该伪代码中，Λ表示随着搜索和遍历过程继续而不断扩展的树，η表示一个节点，其他符号在本文中已定义。

### IV. Level-K博弈论的整合

人类驾驶模型、概率技术以及基于规则的模型已被开发用于自动驾驶中的预测目的。上述方法的复杂性在驾驶员之间出现交互时变得更加显著。此外，车辆间缺乏直接通信进一步增加了现有挑战。因此，需要一种能够考虑人类驾驶员行为特征和策略性的技术。博弈论方法最近在自动驾驶领域获得了关注。在经济学和心理学中，博弈论应用中认知层级理论（cognitive hierarchy theory）描述了智能体在策略博弈中预测其他互动智能体行为的推理深度【5†source】。在Level-K博弈论中，具备特定推理深度的智能体假设其他智能体具备较低的复杂程度，然后据此估计（预测）其他智能体的动作序列，最终在考虑到他人预测动作的基础上执行自身的动作。

具有第 $ k $ 层推理水平的智能体假设其他竞争智能体具有较低的水平（即 $ k-1 $ 层），并预测他们在视界内即将采取的动作序列。在这种假设下，主要智能体预测其他智能体的动作，假设这些智能体的推理水平为 $ k-2 $。这种层级推理一直延续到预测智能体的推理水平降至0，即假设其为不具有策略性的智能体。基于实验研究，人类通常难以达到超过第2层的推理深度。

在本研究中，类似于【5†source】的做法，Level-0的车辆在假设所有周围物体和智能体静止的前提下执行其MCTS。Level-1的车辆假设所有其他车辆均为Level-0。在预测了周围所有车辆的动作序列后，基于预测的交通状态运行Level-1的MCTS。Level-2的车辆遵循相同的模式，假设所有互动车辆均为Level-1。在满足算法终止条件后，MCTS算法识别并输出具有最高奖励的动作。从Level-K理论的角度来看，第 $ l $ 层智能体 $ i $ 采取的策略的累积奖励为：

$$
\Re_l(℘_i) = \sum_{k=0}^m d^k \cdot r_i(a_k, s_k | ℘_{\text{others}})
$$

其中，$ ℘_{\text{others}} $ 为自车对所有周围车辆的预测策略。

### V. 结果与讨论

在本节展示的测试案例中，采样时间 $ \Delta t $ 为0.25秒，规划视界长度为12。折扣因子 $ d $ 设置为0.8，MCTS模拟次数在500到1000之间变化，以评估模拟次数对算法性能的影响。所有算法、函数以及可视化工具均使用C语言编写，并利用多线程技术来提升混合层次多智能体测试案例的计算速度。车辆的期望速度设定为22.35米/秒（即50英里/小时），道路几何结构符合美国高速公路的标准。

表II展示了在不同层次和不同数量的交互车辆条件下，单次自车MCTS执行的计算时间。对于该表格中的数据，无论是自车的轨迹规划还是其他智能体的预测，每次MCTS的迭代次数均设为500。此外，算法在一台普通性能的计算机上执行，系统为Microsoft Windows，CPU为2.6 GHz，内存为8 GB，使用四个线程。所报告的计算时间为五次独立执行的平均值。

| Number of other vehicle | 1  | 2   | 3   | 4   |
| ----------------------- | -- | --- | --- | --- |
| Ego’s level            |    |     |     |     |
| 0                       | 28 | 28  | 28  | 28  |
| 1                       | 55 | 77  | 94  | 117 |
| 2                       | 89 | 134 | 177 | 248 |

大约执行了50个不同初始条件、静止障碍物、车道数量以及决策层次的车辆测试案例。表III展示了具有不同推理层次的车辆在避撞方面的统计数据。与文献【5†source】的表示方式类似，百分比表示特定层次的车辆间避撞事件总数与总交互次数的比率。

| Scenario          | Collision avoidance stats. |
| ----------------- | -------------------------- |
| level-0 & level-0 | 52%                        |
| level-0 & level-1 | 86%                        |
| level-0 & level-2 | 57%                        |
| level-1 & level-1 | 80%                        |
| level-1 & level-2 | 82%                        |
| level-2 & level-2 | 63%                        |

图4显示了在多车道场景下包含多层次智能体的测试案例。Level-0的智能体成功规划了轨迹，在接近静止障碍物时完成了变道，同时保持了期望速度。Level-1智能体表现出谨慎的操作，因为它们假设所有周围智能体均为Level-0。Level-2的智能体在与Level-1智能体互动的过程中也成功完成了变道。Level-2智能体预测其他智能体的运动，假设它们均为Level-1，这与实际交互智能体的指定层次相一致。在该模拟中没有发生碰撞。然而，如果其他智能体的层次为零，则可能会发生碰撞，因为它们会将Level-2智能体视为静止对象。

图5展示了一个包含更多移动和静止车辆的测试案例。尽管在某些时刻（图5的第6部分）某些Level-0和Level-2的智能体几乎发生碰撞，但在该案例中没有发生碰撞，所有智能体成功地规划了它们的运动。此外，Level-1智能体在交互中大多表现出避让行为，而Level-2智能体则执行了较为激进的操作，因为它们能够预测到Level-1智能体的谨慎行为。
