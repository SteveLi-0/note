# 无约束优化

## 1 无约束优化问题

无约束优化问题的定义如下：

\[
\min_{x \in \mathbb{R}^n} f(x)
\]

### 1.1 梯度下降法

梯度下降法是沿着负梯度方向进行优化变量的更新，具体方式为：

\[
x^{k+1} = x^k - \tau \nabla f(x^k)
\]

其中， \(k\) 代表迭代轮数，由于负梯度方向只能保证在 \(x^k\) 的一个局部邻域内目标函数值的下降，所以通常需要搭配一个步长系数 \(\tau\)。如何选择 \(\tau\) 对于优化过程非常重要。

步长系数的选择过程被称为线搜索过程，即沿着优化方向，搜索一个合适的步长的过程。通常分为精确线搜索和非精确线搜索。精确线搜索需要找到沿当前优化方向使得目标函数值最优的步长，然而通常由于效率较低，非精确线搜索只需满足某种条件即可。一般来说，非精确线搜索常用的条件为 **Armijo Condition**，其具体表示形式如下：

\[
f(x^k) - f(x^k + \alpha d) \geq c \cdot \alpha d^T \nabla f(x^k)
\]

其中， \(\alpha\) 代表步长， \(d\) 代表优化方向， \(c\) 代表一个调节参数，通常取 \(c \in (0,1)\)。

梯度下降法在工程上最常用的就是将梯度方向作为优化方向，结合 **Armijo 条件**计算步长。

### 1.2 牛顿法

牛顿法是沿着牛顿方向进行优化变量的更新，牛顿方向是综合考虑局部邻域内的一、二阶微分信息，得到的优化方向，具体形式如下：

\[
x^{k+1} = x^k - [\nabla^2 f(x^k)]^{-1} \nabla f(x^k)
\]

其中， \(\nabla^2 f(x^k)\) 代表 **Hessian 矩阵**。

牛顿法在一些问题上相比于梯度下降法有着更快的收敛速度，但是牛顿法也有一些缺点。比较显然的是牛顿法要求 Hessian 矩阵是**正定**的，另一个是 Hessian 矩阵的获取和**求逆计算可能会非常昂贵**。

Hessian 矩阵非正定的要求是显然的，否刚无法求逆，也就无法求取牛顿方向。Hessian 矩阵的正定主要是因为，如果 Hessian 矩阵不是正定的，无法保证牛顿方向是下降方向。证明方法很容易。首先，我们和负梯度下降法的相同，我们对优化目标函数进行泰勒展开。由于牛顿方向包含了二阶项，若要下降方向成立，必然：

\[
\nabla f(x^k) [\nabla^2 f(x^k)]^{-1} \nabla f(x^k) > 0
\]

所以，**要保证牛顿方向是下降的，必须保证 Hessian 矩阵是正定的。**

实际上，Hessian 矩阵正定通常很难保证。因此，工程实践中会有一些解决办法。当 Hessian 矩阵是半正定时，只需要加上一个很小的单位矩阵可以将 Hessian 矩阵改为正定矩阵，具体改法为：

\[
M = \nabla^2 f(x) + eI, \quad e = \min(1, \|\nabla f(x)\|_\infty)/10
\]

上述改进方法的好处是，当 \(\nabla f(x)\) 接近于 0 时， \(M\) 也会逐渐接近于真实的 Hessian 矩阵。另外，由于改造后的 Hessian 矩阵是正定的，可以采用 Cholesky 分解的方式，来求牛顿方向：

\[
Md = -\nabla f(x), \quad M = LL^T
\]

从而避免 Hessian 矩阵求逆的复杂过程。

当 Hessian 矩阵是不定的时候，可以采用 Bunch-Kaufman 分解，即 \(M = LBL^T = \nabla^2 f(x)\)。其中， \(B\) 是一个 1×1 或 2×2 的块对角矩阵，算法的关键是将这些 1×1 或 2×2 的块中的特征值， 把它们改成正数。

### 1.3 拟牛顿法

牛顿法存在一些缺点：不能保证一定是下降方向，二阶导数信息获取成本较高，需要求解一个线性方程组。拟牛顿法提出的想法是只用到一阶信息去近似用到二阶信息，从而实现超出一阶方法的收敛速度，同时避免求解目标函数的二阶导数。

为了实现这个目标，可以考虑泰勒的泰勒展开：

\[
\nabla f(x+p) \approx \nabla f(x) + \nabla^2 f(x)p
\]

于是，可以得出下面这个方程（也称为 Secant Equation）：

\[
\Delta x^k \approx B^k \Delta g^k
\]

其中，\(\Delta x^k = x^{k+1} - x^k\)， \(\Delta g^k = \nabla f(x^{k+1}) - \nabla f(x^k)\)， \(B^k\) 近似 \([\nabla^2 f(x^k)]^{-1}\)。

然而，满足上述方程的 \(B\) 矩阵有无穷多个，如何选择其中最优的一个呢？可以通过下面这个优化问题得到：

\[
\min_B \| H^{1/2} (B - B^k) H^{1/2} \|^2
\]

其中，满足 \(B = B^T\)，\(\Delta x = B \Delta g\)。

对上述优化问题进行求解，就可以得到大名鼎鼎的拟牛顿法-BFGS 算法的更新过程。

\[
B^{k+1} = \left(I - \frac{\Delta g \Delta x^T}{\Delta g^T \Delta x}\right) B^k \left(I - \frac{\Delta g \Delta x^T}{\Delta g^T \Delta x}\right) + \frac{\Delta x \Delta x^T}{\Delta g^T \Delta x}
\]

这是一个迭代更新的过程，每一轮迭代优化步都需要对 \(B\) 矩阵进行更新，从而用历史的一阶梯度估计二阶信息进行迭代。

遗憾的是，上述的 BFGS 更新过程无法保证 \(B^{k+1}\) 正定，因此不能保证目标函数下降，除非满足不等式：

\[
\Delta g^T \Delta x > 0
\]

为了使该不等式成立，可以在线搜索过程中，在 Armijo 条件的基础上加入新的一个条件，从而得到了 weak Wolfe 条件， 如下：

\[
f(x^k) - f(x^k + \alpha d) \geq -c_1 \alpha d^T \nabla f(x^k)
\]

\[
d^T \nabla f(x^k + \alpha d) \geq c_2 d^T \nabla f(x^k)
\]

遗憾的是，这个条件虽然能保证函数下降，但是却不能保证最终收敛效果。为此，研究人员提出了 BFGS update 的升级版---cautious update，具体如下：

\[
B^{k+1} = \left \{
\begin{aligned}
  &\left(I - \frac{\Delta g \Delta x^T}{\Delta g^T \Delta x}\right) B^k \left(I - \frac{\Delta g \Delta x^T}{\Delta g^T \Delta x}\right) + \frac{\Delta x \Delta x^T}{\Delta g^T \Delta x}, & \text{if } \Delta g^T \Delta x > \epsilon \|\nabla g^T \Delta x\|, \quad \epsilon = 10^{-6}, \\
  &B^k, & \text{otherwise}.
\end{aligned}
\right.
\]

遗憾的是，这个方法依然面临一个问题，就是当 \(x^k\) 距离最优解非常近时，对 \(B^k\) 的估计误差过大，对收敛速度没有什么帮助。这个时候，我们可以利用有限内存的方法，也就是所谓的有限记忆 BFGS（L-BFGS）算法。L-BFGS 算法的细节较为复杂，篇幅有限，这里不再

介绍，感兴趣的朋友可以自行搜索。

### 1.4 牛顿-共轭梯度法

共轭梯度法是一个求解线性方程组 \(Ax = b\) 的高效方法。其中，\(A\) 是一个已知的对称正定矩阵，\(b\) 是一个已知向量。共轭梯度法可以无须知道 \(A\) 中每个元素的具体数值，仅需知道 \(A\xi = b\) 的具体数值，就可以求解得到 \(x\)。

牛顿-共轭梯度法的核心思想是利用共轭梯度方法求线性方程的求解组：\((\nabla^2 f)d = - \nabla f\)，并且，由于共轭梯度法的特点，不需要知道 \(\nabla^2 f\) 中每个元素的具体数值，仅仅需要知道 \(\nabla f\) 的取值即可，其迭代形式和 Hessian 矩阵的求解一样，具体如下：

\[
(\nabla^2 f) \xi \approx \frac{\nabla f(x + \delta \xi) - \nabla f(x)}{\delta}
\]

这便简单的介绍了共轭梯度法求线性方程组的原理。

共轭梯度法通过选择了一组关于对称正定矩阵 \(A\) 共轭的方向向量作为搜索方向，并且在每个搜索方向上做精确线搜索。对于优化问题，这一类优化问题的目标函数最优解为 \(\frac{1}{2} x^T A x + b^T x\)，需要知道的是，只有当 \(A\) 是对称正定的时候，这类优化问题最优解就是 \(A x = b = 0\) 的最优解。

### 1.5 优化算法的收敛速度分析

评价一个优化算法的收敛速度的主要评估的是该算法经过几次迭代能够收敛至最优解附近。

线性收敛的定义为：

\[
\|e^{k+1}\| = C \|e^k\|, \quad C < 1
\]

二次收敛速度：

\[
\|e^{k+1}\| = C \|e^k\|^2
\]

超线性收敛速度定义为：

\[
\|e^{k+1}\| = C \|e^k\|^p, \quad p > 1
\]

最后，借用汪博课程中的一张很经典的图，用来描述各个算法在计算复杂度和收敛速度上的关系：

（图示收敛速度的快慢，横轴表示计算代价大小）

